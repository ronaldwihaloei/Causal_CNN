{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import random\n",
    "import pickle\n",
    "import math\n",
    "\n",
    "from sklift.metrics import qini_auc_score, uplift_auc_score\n",
    "from sklift.viz import plot_qini_curve, plot_uplift_curve\n",
    "\n",
    "torch.manual_seed(2)\n",
    "torch.cuda.manual_seed(2)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "np.random.seed(2)\n",
    "random.seed(2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "torch.cuda.set_device('cuda:3')\n",
    "device = torch.device('cuda:3')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "IDs = ['subject_id', 'hadm_id', 'icustay_id']\n",
    "features = ['age', 'gender', 'bmi', 'apsiii', 'sofa', 'smoker', 'hemoglobin']\n",
    "treatment = 'oxygenation'\n",
    "outcome = 'death90'\n",
    "\n",
    "df = pd.read_csv('df_oxygenation.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "cov = np.float32(np.load('matrix_temporal.npy'))\n",
    "cov = cov / np.max(cov)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_final = df.copy()\n",
    "\n",
    "df_final.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_final.head()\n",
    "df_final.fillna(0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def load_data(X, y):\n",
    "    X = torch.unsqueeze(X, 1)\n",
    "    indices = torch.randperm(y.shape[0])\n",
    "    X = X[indices]\n",
    "    y = y[indices.numpy()]\n",
    "    train_n = int(0.7 * len(y))\n",
    "    return X[:train_n], y[:train_n], X[train_n:], y[train_n:]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def make_pairs(arrays, labels):\n",
    "    pair_arrays = []\n",
    "    pair_labels = []\n",
    "    \n",
    "    idx = np.array(range(0, len(arrays)))\n",
    "    \n",
    "    for index in range(len(arrays)):\n",
    "        current_array = arrays[index]\n",
    "        label = labels[index]\n",
    "\n",
    "        first_idx = np.random.choice(idx)\n",
    "        first_array = arrays[first_idx]\n",
    "        first_label = labels[first_idx]\n",
    "\n",
    "        pair_arrays.append(torch.stack([current_array, first_array], dim=0))\n",
    "        pair_labels.append([abs(label - first_label)])\n",
    "        \n",
    "        second_idx = np.random.choice(idx)\n",
    "        second_array = arrays[second_idx]\n",
    "        second_label = labels[second_idx]\n",
    "        \n",
    "        pair_arrays.append(torch.stack([current_array, second_array], dim=0))\n",
    "        pair_labels.append([abs(label - second_label)])\n",
    "        \n",
    "        third_idx = np.random.choice(idx)\n",
    "        third_array = arrays[third_idx]\n",
    "        third_label = labels[third_idx]\n",
    "        \n",
    "        pair_arrays.append(torch.stack([current_array, third_array], dim=0))\n",
    "        pair_labels.append([abs(label - third_label)])\n",
    "    \n",
    "    # return a 2-tuple of our patient pairs and labels\n",
    "    return (torch.stack(pair_arrays), torch.Tensor(pair_labels))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "k = list(np.arange(1, 11, 1)) + list(np.arange(15, 105, 5))\n",
    "\n",
    "MSE_table = pd.DataFrame(columns=k)\n",
    "Qini_table = pd.DataFrame(columns=k)\n",
    "AUUC_table = pd.DataFrame(columns=k)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Training patients with statin\n",
    "\n",
    "df_final_db_1 = pd.read_csv('pats_db_1_balanced.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "padded_array_1 = torch.load('pats_db_1_balanced.pt').cpu()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "padded_array_1[padded_array_1 != padded_array_1] = 0\n",
    "torch.isnan(padded_array_1).any()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "padded_array_1.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_final_db_1.columns"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X_train, y_train, X_test, y_test = load_data(padded_array_1, df_final_db_1['propensity_score'].to_numpy())\n",
    "\n",
    "(pair_train, label_train) = make_pairs(X_train, y_train)\n",
    "(pair_test, label_test) = make_pairs(X_test, y_test)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "tensor_X_train_1 = pair_train[:, 0]\n",
    "tensor_X_train_2 = pair_train[:, 1]\n",
    "tensor_y_train = label_train[:]\n",
    "train_dataset = TensorDataset(tensor_X_train_1, tensor_X_train_2, tensor_y_train)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=64)\n",
    "\n",
    "tensor_X_test_1 = pair_test[:, 0]\n",
    "tensor_X_test_2 = pair_test[:, 1]\n",
    "tensor_y_test = label_test[:]\n",
    "test_dataset = TensorDataset(tensor_X_test_1, tensor_X_test_2, tensor_y_test)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=64)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self, n_filter, filter1_length, filter2_length, filter3_length, n_visit, n_feature, covariance):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(1, int(n_filter/3), kernel_size=(filter1_length, n_feature), padding=0, stride=1)\n",
    "        self.conv2 = nn.Conv2d(1, int(n_filter/3), kernel_size=(filter2_length, n_feature), padding=0, stride=1)\n",
    "        self.conv3 = nn.Conv2d(1, int(n_filter/3), kernel_size=(filter3_length, n_feature), padding=0, stride=1)\n",
    "        self.activ1 = nn.ReLU(inplace=True)\n",
    "        self.maxpool1 = nn.MaxPool2d((n_visit, 1), stride=(n_visit, 1), padding=(int((filter1_length - 1) / 2), 0))\n",
    "        self.maxpool2 = nn.MaxPool2d((n_visit, 1), stride=(n_visit, 1), padding=(int((filter2_length - 1) / 2), 0))\n",
    "        self.maxpool3 = nn.MaxPool2d((n_visit, 1), stride=(n_visit, 1), padding=(int((filter3_length - 1) / 2), 0))\n",
    "        self.sim_matrix = nn.Parameter(torch.randn((n_filter, n_filter)), requires_grad=True)\n",
    "        self.covariance = nn.Parameter(torch.Tensor(covariance), requires_grad=False)\n",
    "        self.dropout1 = nn.Dropout(0.2)\n",
    "        self.linear1 = nn.Linear(int(2 * n_filter) + 2, 1)\n",
    "        self.sigmoid1 = nn.Sigmoid()\n",
    "\n",
    "    def forward_once(self, x):\n",
    "        zeropad = nn.ZeroPad2d((0,0,1,0))\n",
    "        output1 = self.conv1(x)\n",
    "        output1 = self.activ1(output1)\n",
    "        if (filter1_length % 2) == 0:\n",
    "            output1 = zeropad(output1)\n",
    "        output1 = self.maxpool1(output1)\n",
    "        output1 = output1.view(-1, list(output1.size())[1])\n",
    "        output2 = self.conv2(x)\n",
    "        output2 = self.activ1(output2)\n",
    "        if (filter2_length % 2) == 0:\n",
    "            output2 = zeropad(output2)\n",
    "        output2 = self.maxpool2(output2)\n",
    "        output2 = output2.view(-1, list(output2.size())[1])\n",
    "        output3 = self.conv3(x)\n",
    "        output3 = self.activ1(output3)\n",
    "        if (filter3_length % 2) == 0:\n",
    "            output3 = zeropad(output3)\n",
    "        output3 = self.maxpool3(output3)\n",
    "        output3 = output3.view(-1, list(output3.size())[1])\n",
    "        output = torch.cat((output1, output2, output3), 1)\n",
    "        return output\n",
    "\n",
    "    def forward(self, input1, input2):\n",
    "        output1 = self.forward_once(input1)\n",
    "        output2 = self.forward_once(input2)\n",
    "        sim_temp = torch.matmul(output1, self.sim_matrix)\n",
    "        sim_val1 = (sim_temp * output2).sum(dim=1)\n",
    "        sim_val1 = sim_val1.view((-1, 1))\n",
    "        mean_input1 = torch.mean(input1[:,:,:,:], 2)\n",
    "        mean_input2 = torch.mean(input2[:,:,:,:], 2)\n",
    "        delta = mean_input1 - mean_input2\n",
    "        sim_val2 = torch.sqrt(torch.matmul(torch.matmul(delta, self.covariance.view(-1, self.covariance.size()[0], self.covariance.size()[1])), torch.transpose(delta, 1, 2))).view(-1, 1)\n",
    "        output = torch.cat((output1, output2, sim_val1, sim_val2), 1)\n",
    "        output = self.dropout1(output)\n",
    "        output = self.linear1(output)\n",
    "        output = self.sigmoid1(output)\n",
    "        return output"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "n_filter = 96\n",
    "filter1_length = 8\n",
    "filter2_length = 8\n",
    "filter3_length = 8\n",
    "n_visit = padded_array_1.shape[1]\n",
    "n_feature = padded_array_1.shape[2]\n",
    "\n",
    "net_1 = Net(n_filter, filter1_length, filter2_length, filter3_length, n_visit, n_feature, cov).to(device)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from torchinfo import summary\n",
    "\n",
    "summary(net_1, input_size=((X_train.shape[0], X_train.shape[1], X_train.shape[2], X_train.shape[3]), (X_train.shape[0], X_train.shape[1], X_train.shape[2], X_train.shape[3])))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for name, param in net_1.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(name, param.data, param.size())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "net_1.parameters"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(list(net_1.parameters()))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "import torchmetrics\n",
    "\n",
    "# train the model\n",
    "def train_NET(train_loader, val_loader, run_device, model, epochs=1000):\n",
    "    counter = []\n",
    "    train_loss_history = [] \n",
    "    iteration_number = 0\n",
    "    # define the optimization\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "    # enumerate epochs\n",
    "    for epoch in range(epochs):\n",
    "        train_loss_total = 0\n",
    "        train_steps = 0\n",
    "        train_metric = torchmetrics.MeanSquaredError()\n",
    "        # enumerate mini batches\n",
    "        for i, (input1, input2, label) in enumerate(train_loader):\n",
    "            # put data on gpu\n",
    "            pat1 = input1.to(run_device)\n",
    "            pat2 = input2.to(run_device)\n",
    "            label = label.to(run_device)\n",
    "            # clear the gradients\n",
    "            optimizer.zero_grad()\n",
    "            # compute the model output\n",
    "            yhat = model(pat1, pat2)\n",
    "            # calculate loss\n",
    "            train_loss = criterion(yhat, label)\n",
    "            # calculate accuracy\n",
    "            acc_train = train_metric(yhat.cpu(), label.cpu())\n",
    "            # credit assignment\n",
    "            train_loss.backward()\n",
    "            # update model weights\n",
    "            optimizer.step()\n",
    "            # print statistics\n",
    "            train_loss_total += train_loss.item()\n",
    "            train_steps += 1\n",
    "        print('Epoch number {}\\n Current train loss {}\\n Current train MSE {}\\n'.format(epoch + 1, train_loss_total / train_steps, train_metric.compute()))\n",
    "        iteration_number += 1\n",
    "        counter.append(iteration_number)\n",
    "        train_loss_history.append(train_loss_total / train_steps)\n",
    "        \n",
    "        # validation loss\n",
    "        val_loss_total = 0\n",
    "        val_steps = 0\n",
    "        val_metric = torchmetrics.MeanSquaredError()\n",
    "        for j, (input1, input2, label) in enumerate(val_loader):\n",
    "            with torch.no_grad():\n",
    "                # put data on gpu\n",
    "                pat1 = input1.to(run_device)\n",
    "                pat2 = input2.to(run_device)\n",
    "                label = label.to(run_device)\n",
    "                # compute the model output\n",
    "                yhat = model(pat1, pat2)\n",
    "                # calculate loss\n",
    "                val_loss = criterion(yhat, label)\n",
    "                # calculate accuracy\n",
    "                acc_val = val_metric(yhat.cpu(), label.cpu())\n",
    "                # print statistics\n",
    "                val_loss_total += val_loss.item()\n",
    "                val_steps += 1\n",
    "        print('Epoch number {}\\n Current val loss {}\\n Current val MSE {}\\n'.format(epoch + 1, val_loss_total / val_steps, val_metric.compute()))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_NET(train_dataloader, test_dataloader, device, net_1, 500)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Training patients without statin\n",
    "\n",
    "df_final_db_0 = pd.read_csv('pats_db_0_balanced.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "padded_array_0 = torch.load('pats_db_0_balanced.pt').cpu()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "padded_array_0[padded_array_0 != padded_array_0] = 0\n",
    "torch.isnan(padded_array_0).any()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "padded_array_0.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X_train, y_train, X_test, y_test = load_data(padded_array_0, df_final_db_0['propensity_score'].to_numpy())\n",
    "\n",
    "(pair_train, label_train) = make_pairs(X_train, y_train)\n",
    "(pair_test, label_test) = make_pairs(X_test, y_test)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "tensor_X_train_1 = pair_train[:, 0]\n",
    "tensor_X_train_2 = pair_train[:, 1]\n",
    "tensor_y_train = label_train[:]\n",
    "train_dataset = TensorDataset(tensor_X_train_1, tensor_X_train_2, tensor_y_train)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=64)\n",
    "\n",
    "tensor_X_test_1 = pair_test[:, 0]\n",
    "tensor_X_test_2 = pair_test[:, 1]\n",
    "tensor_y_test = label_test[:]\n",
    "test_dataset = TensorDataset(tensor_X_test_1, tensor_X_test_2, tensor_y_test)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=64)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self, n_filter, filter1_length, filter2_length, filter3_length, n_visit, n_feature, covariance):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(1, int(n_filter/3), kernel_size=(filter1_length, n_feature), padding=0, stride=1)\n",
    "        self.conv2 = nn.Conv2d(1, int(n_filter/3), kernel_size=(filter2_length, n_feature), padding=0, stride=1)\n",
    "        self.conv3 = nn.Conv2d(1, int(n_filter/3), kernel_size=(filter3_length, n_feature), padding=0, stride=1)\n",
    "        self.activ1 = nn.ReLU(inplace=True)\n",
    "        self.maxpool1 = nn.MaxPool2d((n_visit, 1), stride=(n_visit, 1), padding=(int((filter1_length - 1) / 2), 0))\n",
    "        self.maxpool2 = nn.MaxPool2d((n_visit, 1), stride=(n_visit, 1), padding=(int((filter2_length - 1) / 2), 0))\n",
    "        self.maxpool3 = nn.MaxPool2d((n_visit, 1), stride=(n_visit, 1), padding=(int((filter3_length - 1) / 2), 0))\n",
    "        self.sim_matrix = nn.Parameter(torch.randn((n_filter, n_filter)), requires_grad=True)\n",
    "        self.covariance = nn.Parameter(torch.Tensor(covariance), requires_grad=False)\n",
    "        self.dropout1 = nn.Dropout(0.2)\n",
    "        self.linear1 = nn.Linear(int(2 * n_filter) + 2, 1)\n",
    "        self.sigmoid1 = nn.Sigmoid()\n",
    "\n",
    "    def forward_once(self, x):\n",
    "        zeropad = nn.ZeroPad2d((0,0,1,0))\n",
    "        output1 = self.conv1(x)\n",
    "        output1 = self.activ1(output1)\n",
    "        if (filter1_length % 2) == 0:\n",
    "            output1 = zeropad(output1)\n",
    "        output1 = self.maxpool1(output1)\n",
    "        output1 = output1.view(-1, list(output1.size())[1])\n",
    "        output2 = self.conv2(x)\n",
    "        output2 = self.activ1(output2)\n",
    "        if (filter2_length % 2) == 0:\n",
    "            output2 = zeropad(output2)\n",
    "        output2 = self.maxpool2(output2)\n",
    "        output2 = output2.view(-1, list(output2.size())[1])\n",
    "        output3 = self.conv3(x)\n",
    "        output3 = self.activ1(output3)\n",
    "        if (filter3_length % 2) == 0:\n",
    "            output3 = zeropad(output3)\n",
    "        output3 = self.maxpool3(output3)\n",
    "        output3 = output3.view(-1, list(output3.size())[1])\n",
    "        output = torch.cat((output1, output2, output3), 1)\n",
    "        return output\n",
    "\n",
    "    def forward(self, input1, input2):\n",
    "        output1 = self.forward_once(input1)\n",
    "        output2 = self.forward_once(input2)\n",
    "        sim_temp = torch.matmul(output1, self.sim_matrix)\n",
    "        sim_val1 = (sim_temp * output2).sum(dim=1)\n",
    "        sim_val1 = sim_val1.view((-1, 1))\n",
    "        mean_input1 = torch.mean(input1[:,:,:,:], 2)\n",
    "        mean_input2 = torch.mean(input2[:,:,:,:], 2)\n",
    "        delta = mean_input1 - mean_input2\n",
    "        sim_val2 = torch.sqrt(torch.matmul(torch.matmul(delta, self.covariance.view(-1, self.covariance.size()[0], self.covariance.size()[1])), torch.transpose(delta, 1, 2))).view(-1, 1)\n",
    "        output = torch.cat((output1, output2, sim_val1, sim_val2), 1)\n",
    "        output = self.dropout1(output)\n",
    "        output = self.linear1(output)\n",
    "        output = self.sigmoid1(output)\n",
    "        return output"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "n_filter = 96\n",
    "filter1_length = 8\n",
    "filter2_length = 8\n",
    "filter3_length = 8\n",
    "n_visit = padded_array_0.shape[1]\n",
    "n_feature = padded_array_0.shape[2]\n",
    "\n",
    "net_0 = Net(n_filter, filter1_length, filter2_length, filter3_length, n_visit, n_feature, cov).to(device)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from torchinfo import summary\n",
    "\n",
    "summary(net_0, input_size=((X_train.shape[0], X_train.shape[1], X_train.shape[2], X_train.shape[3]), (X_train.shape[0], X_train.shape[1], X_train.shape[2], X_train.shape[3])))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for name, param in net_0.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(name, param.data, param.size())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "net_0.parameters"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(list(net_0.parameters()))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "import torchmetrics\n",
    "\n",
    "# train the model\n",
    "def train_NET(train_loader, val_loader, run_device, model, epochs=1000):\n",
    "    counter = []\n",
    "    train_loss_history = [] \n",
    "    iteration_number = 0\n",
    "    # define the optimization\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "    # enumerate epochs\n",
    "    for epoch in range(epochs):\n",
    "        train_loss_total = 0\n",
    "        train_steps = 0\n",
    "        train_metric = torchmetrics.MeanSquaredError()\n",
    "        # enumerate mini batches\n",
    "        for i, (input1, input2, label) in enumerate(train_loader):\n",
    "            # put data on gpu\n",
    "            pat1 = input1.to(run_device)\n",
    "            pat2 = input2.to(run_device)\n",
    "            label = label.to(run_device)\n",
    "            # clear the gradients\n",
    "            optimizer.zero_grad()\n",
    "            # compute the model output\n",
    "            yhat = model(pat1, pat2)\n",
    "            # calculate loss\n",
    "            train_loss = criterion(yhat, label)\n",
    "            # calculate accuracy\n",
    "            acc_train = train_metric(yhat.cpu(), label.cpu())\n",
    "            # credit assignment\n",
    "            train_loss.backward()\n",
    "            # update model weights\n",
    "            optimizer.step()\n",
    "            # print statistics\n",
    "            train_loss_total += train_loss.item()\n",
    "            train_steps += 1\n",
    "        print('Epoch number {}\\n Current train loss {}\\n Current train MSE {}\\n'.format(epoch + 1, train_loss_total / train_steps, train_metric.compute()))\n",
    "        iteration_number += 1\n",
    "        counter.append(iteration_number)\n",
    "        train_loss_history.append(train_loss_total / train_steps)\n",
    "        \n",
    "        # validation loss\n",
    "        val_loss_total = 0\n",
    "        val_steps = 0\n",
    "        val_metric = torchmetrics.MeanSquaredError()\n",
    "        for j, (input1, input2, label) in enumerate(val_loader):\n",
    "            with torch.no_grad():\n",
    "                # put data on gpu\n",
    "                pat1 = input1.to(run_device)\n",
    "                pat2 = input2.to(run_device)\n",
    "                label = label.to(run_device)\n",
    "                # compute the model output\n",
    "                yhat = model(pat1, pat2)\n",
    "                # calculate loss\n",
    "                val_loss = criterion(yhat, label)\n",
    "                # calculate accuracy\n",
    "                acc_val = val_metric(yhat.cpu(), label.cpu())\n",
    "                # print statistics\n",
    "                val_loss_total += val_loss.item()\n",
    "                val_steps += 1\n",
    "        print('Epoch number {}\\n Current val loss {}\\n Current val MSE {}\\n'.format(epoch + 1, val_loss_total / val_steps, val_metric.compute()))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_NET(train_dataloader, test_dataloader, device, net_0, 500)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def load_data_for_eval(X):\n",
    "    X = torch.unsqueeze(X, 1)\n",
    "    \n",
    "    return X[:]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def compute_uplift(y_treat, y_untreat):\n",
    "    uplift = np.average(y_treat) - np.average(y_untreat)\n",
    "    \n",
    "    return uplift\n",
    "\n",
    "def calculate_mse(y_true, y_pred):\n",
    "    MSE = np.average(np.square(np.array(y_true) - np.array(y_pred)))\n",
    "    \n",
    "    return MSE"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "SEED = 0 # Repeat five times with different seed values\n",
    "\n",
    "df_1visit = pd.read_csv('df_oxygenation.csv')\n",
    "\n",
    "sample = pd.read_csv('sample.csv')\n",
    "print(sample['icustay_id'].nunique())\n",
    "sample = df_1visit[df_1visit['icustay_id'].isin(sample['icustay_id'].unique().tolist())].copy()\n",
    "print(sample['icustay_id'].nunique())\n",
    "sample_for_test_0 = sample[sample.death90 == 0].sample(n=50, random_state=SEED)\n",
    "sample_for_test_1 = sample[sample.death90 == 1].sample(n=50, random_state=SEED, replace=True)\n",
    "sample_for_test = sample_for_test_0.append(sample_for_test_1, ignore_index=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import time\n",
    "import torch.nn.functional as F\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "padded_array_sample = torch.zeros((1, 8, len(features))).to(device)\n",
    "\n",
    "for i in sample_for_test['icustay_id']:\n",
    "    temp_pat = torch.tensor(df_final[df_final.icustay_id == i].copy()[features].values.astype(np.float32)).to(device)\n",
    "    temp_pat = F.pad(temp_pat, pad=(0, 0, 0, 8 - temp_pat.shape[0]))\n",
    "    temp_pat = torch.unsqueeze(temp_pat, 0)\n",
    "    padded_array_sample = torch.vstack((padded_array_sample, temp_pat))\n",
    "    \n",
    "padded_array_sample = padded_array_sample[1:,:,:].type(torch.float32)\n",
    "\n",
    "end = time.time()\n",
    "print(end - start)\n",
    "\n",
    "padded_array_sample"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "padded_array_sample[padded_array_sample != padded_array_sample] = 0\n",
    "torch.isnan(padded_array_sample).any()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X_sample = load_data_for_eval(padded_array_sample)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "padded_array_db_1 = padded_array_1\n",
    "padded_array_db_0 = padded_array_0"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "padded_array_db_1[padded_array_db_1 != padded_array_db_1] = 0\n",
    "print(torch.isnan(padded_array_db_1).any())\n",
    "\n",
    "padded_array_db_0[padded_array_db_0 != padded_array_db_0] = 0\n",
    "print(torch.isnan(padded_array_db_0).any())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X_pats_db_1 = load_data_for_eval(padded_array_db_1)\n",
    "X_pats_db_0 = load_data_for_eval(padded_array_db_0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "tensor_X_sample = X_sample\n",
    "sample_dataset = TensorDataset(tensor_X_sample)\n",
    "sample_dataloader = DataLoader(sample_dataset, batch_size=1)\n",
    "\n",
    "tensor_X_db_1 = X_pats_db_1\n",
    "db_dataset_1 = TensorDataset(tensor_X_db_1)\n",
    "db_dataloader_1 = DataLoader(db_dataset_1, batch_size=1000)\n",
    "\n",
    "tensor_X_db_0 = X_pats_db_0\n",
    "db_dataset_0 = TensorDataset(tensor_X_db_0)\n",
    "db_dataloader_0 = DataLoader(db_dataset_0, batch_size=1000)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "MSE = []\n",
    "Qini = []\n",
    "AUUC = []\n",
    "\n",
    "for no in k:\n",
    "    sample_trans_outcome = []\n",
    "    sample_outcome = []\n",
    "    sample_oxygenation_status = []\n",
    "    sample_uplift = []\n",
    "    \n",
    "    for i, (sample_input, oxygenation_status, true_outcome, trans_outcome) in enumerate(zip(sample_dataloader, sample_for_test[treatment], sample_for_test[outcome], sample_for_test['TransformedOutcome'])):\n",
    "        sim_values_1 = []\n",
    "        sim_values_0 = []\n",
    "    \n",
    "        db_pats_sim_values_1 = df_final_db_1.copy()\n",
    "        db_pats_sim_values_0 = df_final_db_0.copy()\n",
    "    \n",
    "        for j, pat_db_1 in enumerate(db_dataloader_1):             \n",
    "            with torch.no_grad():\n",
    "            \n",
    "                # put data on gpu\n",
    "                pat1 = sample_input[0].to(device)\n",
    "                pat2_1 = pat_db_1[0].to(device)\n",
    "                pat1 = pat1.repeat(pat2_1.size()[0], 1, 1, 1)\n",
    "                # compute the model output, convert to list and add to dictionary\n",
    "                yhat_1 = net_1(pat1, pat2_1).cpu().squeeze().tolist()\n",
    "                sim_values_1 = sim_values_1 + yhat_1\n",
    "            \n",
    "        sim_values_series_1 = pd.Series(sim_values_1, index=db_pats_sim_values_1.index)    \n",
    "        db_pats_sim_values_1['sim_score'] = sim_values_series_1\n",
    "        ordered_patients_1 = db_pats_sim_values_1.sort_values(by = 'sim_score', ascending=True).head(1000)\n",
    "        \n",
    "        for j, (pat_db_0) in enumerate(db_dataloader_0):             \n",
    "            with torch.no_grad():\n",
    "                \n",
    "                # put data on gpu\n",
    "                pat1 = sample_input[0].to(device)\n",
    "                pat2_0 = pat_db_0[0].to(device)\n",
    "                pat1 = pat1.repeat(pat2_0.size()[0], 1, 1, 1)\n",
    "                # compute the model output, convert to list and add to dictionary\n",
    "                yhat_0 = net_0(pat1, pat2_0).cpu().squeeze().tolist()\n",
    "                sim_values_0 = sim_values_0 + yhat_0\n",
    "    \n",
    "        sim_values_series_0 = pd.Series(sim_values_0, index=db_pats_sim_values_0.index)    \n",
    "        db_pats_sim_values_0['sim_score'] = sim_values_series_0\n",
    "        ordered_patients_0 = db_pats_sim_values_0.sort_values(by = 'sim_score', ascending=True).head(1000)\n",
    "        \n",
    "        sample_trans_outcome.append(trans_outcome)\n",
    "        sample_outcome.append(true_outcome)\n",
    "        sample_oxygenation_status.append(oxygenation_status)\n",
    "        \n",
    "        ordered_patients_1_k = ordered_patients_1.head(no)\n",
    "        ordered_patients_0_k = ordered_patients_0.head(no)\n",
    "        \n",
    "        y_1_k = ordered_patients_1_k[outcome].values\n",
    "        y_0_k = ordered_patients_0_k[outcome].values\n",
    "        \n",
    "        uplift_k = compute_uplift(y_1_k, y_0_k)\n",
    "        sample_uplift.append(uplift_k)\n",
    "        \n",
    "    MSE.append(calculate_mse(sample_trans_outcome, sample_uplift))\n",
    "    Qini.append(qini_auc_score(sample_outcome, sample_uplift, sample_oxygenation_status))\n",
    "    AUUC.append(uplift_auc_score(sample_outcome, sample_uplift, sample_oxygenation_status))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "series = pd.Series(MSE, index = MSE_table.columns)\n",
    "MSE_table = MSE_table.append(series, ignore_index=True)\n",
    "MSE_table"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "series = pd.Series(Qini, index = Qini_table.columns)\n",
    "Qini_table = Qini_table.append(series, ignore_index=True)\n",
    "Qini_table"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "series = pd.Series(AUUC, index = AUUC_table.columns)\n",
    "AUUC_table = AUUC_table.append(series, ignore_index=True)\n",
    "AUUC_table"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Deep_learning",
   "language": "python",
   "name": "deep_learning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}